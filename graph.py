import json
import time
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from agents import (
    AgentState, 
    create_supervisor, 
    create_bulgarian_legal_search_agent, 
    create_legal_document_analyzer_agent,
    create_legal_citation_specialist_agent,
    get_members
)

def build_graph():
    supervisor_chain = create_supervisor()
    bulgarian_search_node = create_bulgarian_legal_search_agent()
    document_analyzer_node = create_legal_document_analyzer_agent()
    citation_specialist_node = create_legal_citation_specialist_agent()

    graph_builder = StateGraph(AgentState)
    graph_builder.add_node("Supervisor", supervisor_chain)
    graph_builder.add_node("Bulgarian_Legal_Searcher", bulgarian_search_node)
    graph_builder.add_node("Legal_Document_Analyzer", document_analyzer_node)
    graph_builder.add_node("Legal_Citation_Specialist", citation_specialist_node)

    members = get_members()
    for member in members:
        graph_builder.add_edge(member, "Supervisor")

    conditional_map = {k: k for k in members}
    conditional_map["FINISH"] = END
    
    # Update conditional edges to handle the new tool output format
    def route_supervisor(x):
        if isinstance(x, dict) and "args" in x:
            return x["args"]["next"]
        elif isinstance(x, dict) and "next" in x:
            return x["next"]
        else:
            return END
    
    graph_builder.add_conditional_edges("Supervisor", route_supervisor, conditional_map)
    graph_builder.set_entry_point("Supervisor")

    graph = graph_builder.compile()

    return graph

def visualize_graph():
    """Generate and save graph visualization"""
    graph = build_graph()
    try:
        # Try to create PNG visualization
        png_data = graph.get_graph().draw_mermaid_png()
        with open("graph_visualization.png", "wb") as f:
            f.write(png_data)
        print("Graph visualization saved as 'graph_visualization.png'")
        
        # Also create Mermaid text format
        mermaid_syntax = graph.get_graph().draw_mermaid()
        with open("graph_visualization.md", "w") as f:
            f.write("# Bulgarian Legal Research Agent Graph\n\n")
            f.write("```mermaid\n")
            f.write(mermaid_syntax)
            f.write("\n```")
        print("Mermaid syntax saved as 'graph_visualization.md'")
        
        return mermaid_syntax
        
    except Exception as e:
        print(f"Could not generate PNG visualization: {e}")
        # Fallback to text representation
        mermaid_syntax = graph.get_graph().draw_mermaid()
        print("Mermaid Graph Structure:")
        print(mermaid_syntax)
        return mermaid_syntax

def run_graph(input_message, config=None):
    """Run the Bulgarian legal research graph with optional configuration"""
    graph = build_graph()
    
    # Default configuration
    default_config = {
        "query_depth": "medium",
        "complexity_level": "standard", 
        "max_iterations": 3,
        "context_window": 5000,
        "crawling_depth": 2,
        "focus_domains": ["ciela.net", "apis.bg", "lakorda.com"]
    }
    
    if config:
        default_config.update(config)
    
    # Enhanced response processing with configuration
    response = graph.invoke({
        "messages": [HumanMessage(content=input_message)],
        "config": default_config
    })

    # Extract and format the content with better handling
    if not response.get('messages'):
        return "ĞÑĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ°Ğ½ Ğ¾Ñ‚Ğ³Ğ¾Ğ²Ğ¾Ñ€."
    
    # Get the last message content
    last_message = response['messages'][-1]
    content = last_message.content if hasattr(last_message, 'content') else str(last_message)

    # Extract sources from all messages for top sources formatting
    all_sources = []
    for msg in response['messages']:
        if hasattr(msg, 'content'):
            msg_content = msg.content
            # Look for sources in the message content 
            if isinstance(msg_content, list):
                for item in msg_content:
                    if isinstance(item, dict) and 'title' in item and 'href' in item:
                        all_sources.append(item)

    # Format the response with top sources at the beginning if not already formatted
    if "ğŸ“š **Ğ¢ĞĞŸ 5 ĞĞĞ™-Ğ Ğ•Ğ›Ğ•Ğ’ĞĞĞ¢ĞĞ˜ Ğ˜Ğ—Ğ¢ĞĞ§ĞĞ˜Ğ¦Ğ˜**" not in content and all_sources:
        top_sources = format_top_sources(all_sources[:5])
        result = top_sources + content
    else:
        result = content

    # Add metadata about the processing
    result += f"\n\n---\n*ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞµĞ½Ğ¾ Ğ¾Ñ‚ {len(response['messages'])} Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ*"
    result += f"\n*ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ: {default_config['query_depth']} Ğ´ÑŠĞ»Ğ±Ğ¾Ñ‡Ğ¸Ğ½Ğ°, {default_config['complexity_level']} ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚*"
    
    return result

def run_graph_with_streaming(input_message, progress_callback=None, config=None):
    """Run graph with streaming for better user experience and progress tracking"""
    graph = build_graph()
    
    # Default configuration
    default_config = {
        "query_depth": "medium",
        "complexity_level": "standard", 
        "max_iterations": 3,
        "context_window": 5000,
        "crawling_depth": 2,
        "focus_domains": ["ciela.net", "apis.bg", "lakorda.com"]
    }
    
    if config:
        default_config.update(config)
    
    print("ğŸ¤– Ğ—Ğ°Ğ¿Ğ¾Ñ‡Ğ²Ğ°Ğ¼ Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¾ Ğ¸Ğ·ÑĞ»ĞµĞ´Ğ²Ğ°Ğ½Ğµ...\n")
    
    events_processed = 0
    total_steps = default_config['max_iterations'] * 3  # Estimate based on agents
    
    try:
        for event in graph.stream({
            "messages": [HumanMessage(content=input_message)],
            "config": default_config
        }):
            for key, value in event.items():
                if key != "__end__":
                    events_processed += 1
                    progress = min(events_processed / total_steps, 1.0)
                    
                    # Enhanced progress tracking
                    if progress_callback:
                        if "Bulgarian_Legal_Searcher" in key:
                            progress_callback(f"ğŸ” Ğ¢ÑŠÑ€ÑÑ Ğ² Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ±Ğ°Ğ·Ğ¸ Ğ´Ğ°Ğ½Ğ½Ğ¸...", progress)
                        elif "Legal_Document_Analyzer" in key:
                            progress_callback(f"ğŸ“š ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸...", progress)
                        elif "Legal_Citation_Specialist" in key:
                            progress_callback(f"ğŸ“– Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ°Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ĞµĞ½ Ğ¾Ñ‚Ğ³Ğ¾Ğ²Ğ¾Ñ€...", progress)
                        elif "Supervisor" in key:
                            progress_callback(f"âš–ï¸ ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ°Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·...", progress)
                    
                    print(f"ğŸ“Š **{key}**: {value}")
                    print("---")
                    time.sleep(0.1)  # Small delay for better user experience
        
        # Get final result
        final_response = graph.invoke({
            "messages": [HumanMessage(content=input_message)],
            "config": default_config
        })
        
        return run_graph(input_message, config)  # Use the existing formatting
        
    except Exception as e:
        print(f"Ğ“Ñ€ĞµÑˆĞºĞ° Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼Ğµ Ğ½Ğ° ÑÑ‚Ñ€Ğ¸Ğ¹Ğ¼Ğ¸Ğ½Ğ³ Ğ¸Ğ·Ğ¿ÑŠĞ»Ğ½ĞµĞ½Ğ¸ĞµÑ‚Ğ¾: {e}")
        return run_graph(input_message, config)  # Fallback to regular execution

def get_graph_info():
    """Get information about the Bulgarian legal graph structure"""
    graph = build_graph()
    graph_dict = graph.get_graph()
    
    info = {
        "nodes": list(graph_dict.nodes.keys()),
        "edges": [(edge.source, edge.target) for edge in graph_dict.edges],
        "entry_point": "Supervisor",
        "specialized_agents": [
            "Bulgarian_Legal_Searcher - Ğ•ĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ Ğ² Ğ‘Ğ“ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ±Ğ°Ğ·Ğ¸",
            "Legal_Document_Analyzer - ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸", 
            "Legal_Citation_Specialist - Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚ Ğ¿Ğ¾ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ°Ğ½Ğµ"
        ],
        "supported_domains": ["ciela.net", "apis.bg", "lakorda.com"],
        "legal_areas": ["Ğ“Ñ€Ğ°Ğ¶Ğ´Ğ°Ğ½ÑĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾", "ĞĞ°ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾", "ĞĞ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾", "Ğ¢ÑŠÑ€Ğ³Ğ¾Ğ²ÑĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾", "Ğ¢Ñ€ÑƒĞ´Ğ¾Ğ²Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¾"],
        "tools_available": [
            "google_cse_search - Google Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸",
            "bulgarian_legal_search - Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ°Ğ½Ğ¾ Ğ‘Ğ“ Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ",
            "legal_precedent_search - Ğ¢ÑŠÑ€ÑĞµĞ½Ğµ Ğ½Ğ° ÑÑŠĞ´ĞµĞ±Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°",
            "process_content - ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸"
        ]
    }
    
    return info

def get_config_options():
    """Get available configuration options for the legal research system"""
    return {
        "query_depth": {
            "shallow": "ĞŸĞ¾Ğ²ÑŠÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ - Ğ±ÑŠÑ€Ğ·Ğ¸ Ñ€ĞµĞ·ÑƒĞ»Ñ‚Ğ°Ñ‚Ğ¸",
            "medium": "Ğ¡Ñ€ĞµĞ´Ğ½Ğ° Ğ´ÑŠĞ»Ğ±Ğ¾Ñ‡Ğ¸Ğ½Ğ° - Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ°Ğ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ", 
            "deep": "Ğ”ÑŠĞ»Ğ±Ğ¾ĞºĞ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ - Ğ¸Ğ·Ñ‡ĞµÑ€Ğ¿Ğ°Ñ‚ĞµĞ»ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·"
        },
        "complexity_level": {
            "basic": "ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ - Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑÑĞ½ĞµĞ½Ğ¸Ñ",
            "standard": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ - Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·",
            "expert": "Ğ•ĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ - Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾ ÑÑŠĞ´ÑŠÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ"
        },
        "max_iterations": {
            1: "Ğ‘ÑŠÑ€Ğ·Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ (1 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ)",
            2: "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ (2 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)", 
            3: "Ğ—Ğ°Ğ´ÑŠĞ»Ğ±Ğ¾Ñ‡ĞµĞ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ (3 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)",
            4: "Ğ˜Ğ·Ñ‡ĞµÑ€Ğ¿Ğ°Ñ‚ĞµĞ»Ğ½Ğ¾ Ñ‚ÑŠÑ€ÑĞµĞ½Ğµ (4 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)"
        },
        "context_window": {
            2000: "ĞœĞ°Ğ»ÑŠĞº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (2K Ğ´ÑƒĞ¼Ğ¸)",
            5000: "Ğ¡Ñ€ĞµĞ´ĞµĞ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (5K Ğ´ÑƒĞ¼Ğ¸)",
            8000: "Ğ“Ğ¾Ğ»ÑĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (8K Ğ´ÑƒĞ¼Ğ¸)",
            10000: "ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ĞµĞ½ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (10K Ğ´ÑƒĞ¼Ğ¸)"
        },
        "crawling_depth": {
            1: "Ğ¡Ğ°Ğ¼Ğ¾ Ğ¿ÑŠÑ€Ğ²Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ Ğ½Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸",
            2: "Ğ”Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¿Ñ€Ğ°Ñ‚ĞºĞ¸",
            3: "Ğ”Ğ¾ Ñ‚Ñ€ĞµÑ‚Ğ¾ Ğ½Ğ¸Ğ²Ğ¾ Ğ½Ğ° Ğ¿Ñ€ĞµĞ¿Ñ€Ğ°Ñ‚ĞºĞ¸"
        }
    }

# Helper function to format sources for the response
def format_top_sources(sources, limit=5):
    """Format top sources with metadata for the response"""
    if not sources or not isinstance(sources, list):
        return "ğŸ“š **Ğ¢ĞĞŸ 5 ĞĞĞ™-Ğ Ğ•Ğ›Ğ•Ğ’ĞĞĞ¢ĞĞ˜ Ğ˜Ğ—Ğ¢ĞĞ§ĞĞ˜Ğ¦Ğ˜**\n\nĞÑĞ¼Ğ° Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ½Ğ¸ Ğ¸Ğ·Ñ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ†Ğ¸.\n\n"
    
    formatted = "ğŸ“š **Ğ¢ĞĞŸ 5 ĞĞĞ™-Ğ Ğ•Ğ›Ğ•Ğ’ĞĞĞ¢ĞĞ˜ Ğ˜Ğ—Ğ¢ĞĞ§ĞĞ˜Ğ¦Ğ˜**\n\n"
    
    for i, source in enumerate(sources[:limit], 1):
        if isinstance(source, dict):
            title = source.get('title', 'Ğ‘ĞµĞ· Ğ·Ğ°Ğ³Ğ»Ğ°Ğ²Ğ¸Ğµ')
            href = source.get('href', '#')
            body = source.get('body', '')
            domain = source.get('source_domain', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚ĞµĞ½ Ğ¸Ğ·Ñ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº')
            
            # Extract key information for metadata  
            snippet = body[:80] + "..." if len(body) > 80 else body
            
            formatted += f"**{i}. [{title}]({href})**\n"
            formatted += f"   ğŸ›ï¸ *{domain}*\n" 
            formatted += f"   ğŸ“„ {snippet}\n\n"
    
    formatted += "---\n\n"
    return formatted

def extract_sources_from_response(response_messages):
    """Extract sources from response messages for formatting"""
    sources = []
    
    for message in response_messages:
        content = message.content if hasattr(message, 'content') else str(message)
        
        # Look for source patterns in the content
        import re
        
        # Pattern to match source dictionaries or lists
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and 'title' in item and 'href' in item:
                    sources.append(item)
        elif hasattr(content, 'sources') or 'title' in str(content):
            # Try to extract sources from content
            continue
    
    return sources